---
title: "Text analysis using quanteda and RcppKagome"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
header-includes:
  - \usepackage[utf8]{inputenc}
vignette: >
  %\VignetteIndexEntry{Text analysis using quanteda and RcppKagome}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)
#### prepare directories for cache ---
if (!dir.exists("cache")) dir.create("cache")
if (!dir.exists("cache/ldavis")) dir.create("cache/ldavis")
#### load packages ---
library(tidyverse)
```

## この記事について

[{quanteda}](https://github.com/quanteda/quanteda)と[{RcppKagome}](https://github.com/paithiov909/RcppKagome)を用いたテキストマイニングの例です（[{googledrive}](https://github.com/tidyverse/googledrive)を利用して自作の文章を分析していた過去記事については[Qiitaのログ](https://qiita.com/paithiov909/items/a47a097836e8a9ec12ef/revisions)（revision < 10）から参照してください）。

なお、以下のパッケージについては、ここではGitHubからインストールできるものを使っています。

- [uribo/zipangu](https://github.com/uribo/zipangu)
- [paithiov909/ldccr](https://github.com/paithiov909/ldccr)
- [paithiov909/RcppKagome](https://github.com/paithiov909/RcppKagome)

## データの準備

テキストデータとして[livedoorニュースコーパス](https://www.rondhuit.com/download.html#ldcc)を使います。以下の9カテゴリです。

- トピックニュース
- Sports Watch
- ITライフハック
- 家電チャンネル
- MOVIE ENTER
- 独女通信
- エスマックス
- livedoor HOMME
- Peachy

[{ldccr}](https://github.com/paithiov909/ldccr)でデータフレームにします。

```{r tidy_data_1}
data <- ldccr::parse_ldcc(exdir = "cache")
```

このうち一部だけをquantedaのコーパスオブジェクトとして格納し、いろいろ試していきます。

```{r tidy_data_2}
corp <- data %>%
  dplyr::sample_frac(size = .1)

corp <- corp %>%
  dplyr::pull("body") %>%
  stringr::str_remove_all("[[:punct:]]+") %>%
  zipangu::str_jnormalize() %>%
  RcppKagome::kagome() %>%
  RcppKagome::pack_list() %>%
  dplyr::bind_cols(corp) %>%
  quanteda::corpus()
```

## ワードクラウド

ストップワードとして`rtweet::stopwordslangs`を利用しています。

```{r wordcloud}
stopwords <- rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "category") %>%
  quanteda::dfm_trim(min_termfreq = 10L) %>%
  quanteda.textplots::textplot_wordcloud(color = viridis::cividis(8L))
```

## 出現頻度の集計

```{r stats}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight("prop") %>%
  quanteda::textstat_frequency(groups = "category") %>%
  dplyr::top_n(-30L, rank) %>%
  ggpubr::ggdotchart(
    x = "feature",
    y = "frequency",
    group = "group",
    color = "group",
    rotate = TRUE
  ) +
  ggplot2::theme_bw()
```

## Keyness

ITライフハック（`it-life-hack`）グループの文書とその他の対照を見ています。

```{r keyness}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "category") %>%
  quanteda.textstats::textstat_keyness(target = "it-life-hack") %>%
  quanteda.textplots::textplot_keyness()
```

## 対応分析

全部をプロットすると潰れて見えないので一部だけを抽出しています。

```{r ca}
corp_sample <- quanteda::corpus_sample(corp, size = 32L)
corp_sample %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda.textmodels::textmodel_ca() %>%
  quanteda.textplots::textplot_scale1d(
    margin = "documents",
    groups = quanteda::docvars(corp_sample, "category")
  )
```

## 共起ネットワーク

```{r network}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "category") %>%
  quanteda::dfm_trim(min_termfreq = 100L) %>%
  quanteda::fcm() %>%
  quanteda.textplots::textplot_network(min_freq = .96)
```

## クラスタリング

マンハッタン距離、ward法（ward.D2）です。ここでも一部だけを抽出しています。

```{r clust}
d <- corp_sample %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda.textstats::textstat_dist(method = "manhattan") %>%
  as.dist() %>%
  hclust(method = "ward.D2") %>%
  ggdendro::dendro_data(type = "rectangle") %>%
  purrr::list_modify(
    labels = dplyr::bind_cols(
      .$labels,
      names = names(corp_sample),
      category = quanteda::docvars(corp_sample, "category")
    )
  )

ggplot2::ggplot(ggdendro::segment(d)) +
  ggplot2::geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  ggplot2::geom_text(ggdendro::label(d), mapping = aes(x, y, label = names, colour = category, hjust = 0), size = 3) +
  ggplot2::coord_flip() +
  ggplot2::scale_y_reverse(expand = c(.2, 0)) +
  ggdendro::theme_dendro()
```

## LDA（Latent Dirichlet Allocation）

```{r lda_1}
dtm <- corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_tfidf()

features <- corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::ntoken()

m <- dtm %>%
  as("dgCMatrix") %>%
  textmineR::FitLdaModel(k = 9, iterations = 200, burnin = 175)

m$phi %>%
  textmineR::GetTopTerms(15L) %>%
  knitr::kable()
```

LDAvisで可視化してみます。ただ、LDAvisはもうしばらくメンテナンスされていないパッケージで、ちょっと挙動があやしいところがあります。たとえば、デフォルトロケールがCP932であるWindows環境の場合、`LDAvis::createJSON`で書き出されるラベル（vocab）のエンコーディングがそっちに引きずられてCP932になってしまうため、ブラウザで表示したときにラベルが文字化けします。書き出されたlda.jsonをUTF-8に変換すれば文字化けは解消されるので、とりあえずあとから変換して上書きするとよいです。

```{r lda_2}
LDAvis::createJSON(
  phi = m$phi,
  theta = m$theta,
  doc.length = features,
  vocab = stringi::stri_enc_toutf8(dtm@Dimnames$features),
  term.frequency = quanteda::colSums(dtm)
) %>%
  LDAvis::serVis(open.browser = FALSE, out.dir = file.path(getwd(), "cache/ldavis"))

readr::read_lines_raw(file.path(getwd(), "cache/ldavis", "lda.json")) %>%
  iconv(from = "CP932", to = "UTF-8") %>%
  jsonlite::parse_json(simplifyVector = TRUE) %>%
  jsonlite::write_json(file.path(getwd(), "cache/ldavis", "lda.json"), dataframe = "columns", auto_unbox = TRUE)
```

```{r copy_ldavis_dir, include=FALSE}
files <- list.files(file.path("cache/ldavis"), full.names = TRUE)
for (file in files) {
  file.copy(
    from = file,
    to = file.path(getwd(), "../docs/ldavis")
  )
}
```

> [LDAvis](https://paithiov909.github.io/RcppKagome/ldavis/index.html)

## GloVe

ここでは50次元の埋め込みを得ます。

```{r glove}
toks <- corp %>%
  quanteda::tokens(what = "word") %>%
  as.list() %>%
  text2vec::itoken()

vocab <- toks %>%
  text2vec::create_vocabulary() %>%
  text2vec::prune_vocabulary(term_count_min = 10L)

vectorize <- text2vec::vocab_vectorizer(vocab)

tcm <- text2vec::create_tcm(
  it = toks,
  vectorizer = vectorize,
  skip_grams_window = 5L
)

glove <- text2vec::GlobalVectors$new(
  rank = 50,
  x_max = 15L
)

wv <- glove$fit_transform(
  x = tcm,
  n_iter = 10L
) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  tibble::as_tibble(.name_repair = "minimal", rownames = NA)
```

[{umap}](https://github.com/tkonopka/umap)で次元を減らして可視化します。色は`kmeans`でクラスタリング（ユークリッド距離）して付けています。

```{r umap}
pull_layout <- function(tbl) {
  umap <- umap::umap(as.matrix(tbl))
  layout <- umap$layout
  rownames(layout) <- rownames(tbl)
  return(as.data.frame(layout))
}

vec <- vocab %>%
  dplyr::anti_join(
    y = tibble::tibble(words = stopwords),
    by = c("term" = "words")
  ) %>%
  dplyr::arrange(desc(term_count)) %>%
  dplyr::slice_head(n = 100L) %>%
  dplyr::left_join(tibble::rownames_to_column(wv), by = c("term" = "rowname")) %>%
  tibble::column_to_rownames("term") %>%
  dplyr::select(starts_with("V"))

dist <- proxyC::simil(as(as.matrix(vec), "dgCMatrix"), method = "cosine")
clust <- kmeans(x = dist, centers = 9)
vec <- pull_layout(vec) %>%
  tibble::rownames_to_column() %>%
  dplyr::mutate(cluster = as.factor(clust$cluster))

vec %>%
  ggplot2::ggplot(aes(x = V1, y = V2, colour = cluster)) +
  ggplot2::geom_point() +
  ggrepel::geom_text_repel(aes(label = rowname)) +
  ggplot2::theme_light()
```

## セッション情報

```{r sessioninfo}
sessioninfo::session_info()
```

