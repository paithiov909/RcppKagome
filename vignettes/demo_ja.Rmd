---
title: "Practice: Text analysis using quanteda"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
header-includes:
  - \usepackage[utf8]{inputenc}
vignette: >
  %\VignetteIndexEntry{Practice: Text analysis using quanteda}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)
require(tidyverse)
googledrive::drive_auth(cache = ".secrets")
```

## Googleドキュメントの読み込み

これまで自分が書いてきた文章について、テキスト解析をしてみます。ここで分析している文章はnoteで読むことができます。

> [さちこ｜note](https://note.com/shinabitanori)

これまでに書いた文章はいくつかの場所にバックアップを取っていて、Googleドキュメントにもバックアップがあります。今回はそれらをgoogledriveで取得し、readtextで読み込みます。

`googledrive::drive_download`はディレクトリごとダウンロードしたりはできないようなので、特定のディレクトリにあるファイルのリスト（dribble）を`dplyr::rowwise`で行ごとに渡して`dplyr::do`のなかでダウンロードしています。


```{r drive_download}
aquarium <- googledrive::drive_ls("Archives/aquarium/") %>%
  dplyr::rowwise() %>%
  dplyr::do(
    googledrive::drive_download(
      googledrive::as_id(.$id),
      path = file.path(tempdir(), .$name),
      overwrite = TRUE,
      verbose = FALSE
    )
  )
shinabitanori <- googledrive::drive_ls("Archives/shinabitanori/") %>%
  dplyr::rowwise() %>%
  dplyr::do(
    googledrive::drive_download(
      googledrive::as_id(.$id),
      path = file.path(tempdir(), .$name),
      overwrite = TRUE,
      verbose = FALSE
    )
  )
```

ダウンロードしたdocxファイルのリストをデータフレームとして持っておきます。文章は公開されている場所などに応じて２つのディレクトリに分けて保存されています。ここでは、この保存されているディレクトリを文書の変数として持つようにします。

```{r tidy_data_1}
df <- list("aquarium", "shinabitanori") %>%
  purrr::map_dfr(~
  dplyr::mutate(rlang::eval_tidy(sym(.)), doc = .)) %>%
  dplyr::select(doc, name, local_path) %>%
  tibble::rowid_to_column()

head(df, 8L)
```

## 形態素解析

[この自作パッケージ](https://github.com/paithiov909/RcppKagome)を使っています。結果をquantedaのコーパスオブジェクトとして格納して、いろいろ試していきます。

```{r tidy_data_2}
normalize <- function(str) {
  str %>%
    stringr::str_replace_all("\u2019", "\'") %>%
    stringr::str_replace_all("\u201d", "\"") %>%
    stringr::str_replace_all("[\u02d7\u058a\u2010\u2011\u2012\u2013\u2043\u207b\u208b\u2212]", "-") %>%
    stringr::str_replace_all("[\ufe63\uff0d\uff70\u2014\u2015\u2500\u2501\u30fc]", enc2utf8("\u30fc")) %>%
    stringr::str_replace_all("[~\u223c\u223e\u301c\u3030\uff5e]", "~") %>%
    stringr::str_remove_all("[[:punct:]]+") %>%
    stringr::str_remove_all("[[:blank:]]+") %>%
    stringr::str_remove_all("[[:cntrl:]]+") %>%
    return()
}

corp <- df %>%
  dplyr::rowwise() %>%
  dplyr::do(
    readtext::readtext(
      .$local_path,
      docvarsfrom = "filenames",
      docvarnames = c("name")
  )) %>%
  dplyr::bind_rows() %>%
  dplyr::left_join(
    dplyr::select(df, rowid, doc, name),
    by = "name"
  ) %>%
  dplyr::mutate(text = normalize(text))

corp <- corp %>%
  dplyr::pull("text") %>%
  RcppKagome::kagome() %>%
  RcppKagome::prettify() %>%
  RcppKagome::pack() %>%
  dplyr::bind_cols(corp) %>%
  quanteda::corpus(text_field = "Text")
```

## ワードクラウド

ストップワードとして`rtweet::stopwordslangs`を利用しています。

```{r wordcloud}
stopwords <- rtweet::stopwordslangs %>%
  dplyr::filter(lang == "ja") %>%
  dplyr::filter(p >= .98) %>%
  dplyr::pull(word)

corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 3L) %>%
  quanteda::textplot_wordcloud(comparison = TRUE, color = viridis::cividis(3))
```

## 出現頻度の集計

```{r stats}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight("prop") %>%
  quanteda::textstat_frequency(groups = "doc") %>%
  dplyr::top_n(-16L, rank) %>%
  ggpubr::ggdotchart(
    x = "feature",
    y = "frequency",
    group = "group",
    color = "group",
    rotate = TRUE
  ) +
  ggplot2::theme_bw()
```

## Keyness

aquariumグループの文書とその他の対照を見ています。

```{r keyness}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::textstat_keyness(target = "aquarium") %>%
  quanteda::textplot_keyness()
```

## 対応分析

```{r ca}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda.textmodels::textmodel_ca() %>%
  quanteda.textmodels::textplot_scale1d(
    margin = "documents",
    groups = quanteda::docvars(corp, "doc")
  )
```

## 共起ネットワーク

```{r network}
corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "doc") %>%
  quanteda::dfm_trim(min_termfreq = 20L) %>%
  quanteda::fcm() %>%
  quanteda::textplot_network(min_freq = .8)
```

## クラスタリング

マンハッタン距離、ward法（ward.D2）です。

```{r clust}
d <- corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_weight(scheme = "prop") %>%
  quanteda::textstat_dist(method = "manhattan") %>%
  as.dist() %>%
  hclust(method = "ward.D2") %>%
  ggdendro::dendro_data(type = "rectangle") %>%
  purrr::list_modify(
    labels = dplyr::bind_cols(
      .$labels,
      names = quanteda::docvars(corp, "name"),
      doc = quanteda::docvars(corp, "doc")
    )
  )

ggplot2::ggplot(ggdendro::segment(d)) +
  ggplot2::geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  ggplot2::geom_text(ggdendro::label(d), mapping = aes(x, y, label = names, colour = doc, hjust = 0), size = 3) +
  ggplot2::coord_flip() +
  ggplot2::scale_y_reverse(expand = c(.2, 0)) +
  ggdendro::theme_dendro()
```

## LDA（Latent Dirichlet Allocation）

```{r lda_1}
dtm <- corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm() %>%
  quanteda::dfm_tfidf()

features <- corp %>%
  quanteda::tokens(what = "word") %>%
  quanteda::tokens_remove(stopwords, valuetype = "fixed") %>%
  quanteda::dfm(groups = "name") %>%
  quanteda::ntoken()

m <- dtm %>%
  as("dgCMatrix") %>%
  textmineR::FitLdaModel(k = 3, iterations = 200, burnin = 175)

m$phi %>%
  textmineR::GetTopTerms(15L) %>%
  knitr::kable()
```

LDAvisで可視化してみます。ただ、LDAvisはもうしばらくメンテナンスされていないパッケージで、ちょっと挙動があやしいところがあります。たとえば、デフォルトロケールがCP932であるWindows環境の場合、`LDAvis::createJSON`で書き出されるラベル（vocab）のエンコーディングがそっちに引きずられてCP932になってしまうため、ブラウザで表示したときにラベルが文字化けします。書き出されたlda.jsonをUTF-8に変換すれば文字化けは解消されるので、とりあえずあとから変換して上書きするとよいです。

```{r lda_2}
LDAvis::createJSON(
  phi = m$phi,
  theta = m$theta,
  doc.length = features,
  vocab = stringi::stri_enc_toutf8(dtm @ Dimnames$features),
  term.frequency = quanteda::colSums(dtm)
) %>%
  LDAvis::serVis(open.browser = FALSE, out.dir = file.path(getwd(), "cache"))

readr::read_lines_raw(file.path(getwd(), "cache", "lda.json")) %>%
  iconv(from = "CP932", to = "UTF-8") %>%
  jsonlite::parse_json(simplifyVector = TRUE) %>%
  jsonlite::write_json(file.path(getwd(), "cache", "lda.json"), dataframe = "columns", auto_unbox = TRUE)
```

## GloVe

```{r glove}
toks <- corp %>%
  quanteda::tokens(what = "word") %>%
  as.list() %>%
  text2vec::itoken()

vocab <- toks %>%
  text2vec::create_vocabulary() %>%
  text2vec::prune_vocabulary(term_count_min = 5L)

vect <- text2vec::vocab_vectorizer(vocab)

tcm <- text2vec::create_tcm(
  it = toks,
  vectorizer = vect,
  skip_grams_window = 5L
)

glove <- text2vec::GlobalVectors$new(
  rank = 50,
  x_max = 15L
)

wv <- glove$fit_transform(
  x = tcm,
  n_iter = 10L
) %>%
  as.data.frame(stringsAsFactors = FALSE) %>%
  tibble::as_tibble(.name_repair = "minimal", rownames = NA)
```

Rtsneで次元を減らして可視化します。

```{r tsne}
getRtsneAsTbl <- function(tbl, dim = 2, perp = 30) {
  tsn <- tbl %>% Rtsne::Rtsne(dim = dim, perplexity = perp)
  tsny <- tsn$Y
  rownames(tsny) <- row.names(tbl)
  tsny <- as.data.frame(tsny, stringsAsFactors = FALSE)
  return(tibble::as_tibble(tsny, .name_repair = "minimal", rownames = NA))
}

vec <- vocab %>%
  dplyr::anti_join(
    y = tibble::tibble(words = stopwords),
    by = c("term" = "words")
  ) %>%
  dplyr::arrange(desc(term_count)) %>%
  head(100) %>%
  dplyr::left_join(tibble::rownames_to_column(wv), by = c("term" = "rowname")) %>%
  tibble::column_to_rownames("term") %>%
  dplyr::select(V1, V2)

dist <- proxy::dist(x = vec, y = vec, method = "Euclidean", diag = TRUE)
clust <- kmeans(x = dist, centers = 5)
vec <- getRtsneAsTbl(vec, perp = 2) %>%
  tibble::rownames_to_column() %>%
  dplyr::mutate(cluster = as.factor(clust$cluster))

vec %>%
  ggplot2::ggplot(aes(x = V1, y = V2, colour = cluster)) +
  ggplot2::geom_point() +
  ggrepel::geom_text_repel(aes(label = rowname)) +
  ggplot2::theme_light()
```

## セッション情報

```{r sessioninfo}
sessioninfo::session_info()
```

